# LLM-Архитектура для извлечения сущностей из сценариев

## Обзор

Система была полностью переработана для использования GPU-оптимизированной LLM-архитектуры вместо хардкодной эвристики. Теперь весь процесс извлечения сущностей из сценариев выполняется через многоэтапный LLM-пайплайн.

## Основные изменения

### 1. Полный отказ от хардкодной эвристики
- Удалены все жестко закодированные правила (if "улица" in text)
- Удалены списки ключевых слов для локаций, реквизита и т.д.
- Все извлечение теперь выполняется через LLM

### 2. Многоэтапный процесс обработки

#### Шаг 1: Детекция сцен
LLM анализирует весь сценарий и точно разделяет его на сцены, извлекая:
- Номер сцены
- Заголовок сцены (локация и время суток)
- Границы сцены в тексте

#### Шаг 2: Извлечение сущностей
Для каждой обнаруженной сцены запускается отдельный LLM-запрос для извлечения полного набора данных согласно выбранному пресету.

### 3. Гибкая конфигурация

Все типы извлекаемых сущностей определяются в `config.yaml`:
- Пресеты ("basic", "extended", "full")
- Кастомные наборы сущностей
- Динамическая генерация промптов на основе конфига

### 4. GPU-оптимизация

- Батчевая обработка сцен для эффективного использования GPU
- Поддержка vllm (опционально) для максимальной производительности
- Использование transformers с 4-bit quantization для экономии памяти
- Автоматическое определение устройства (CUDA/CPU)

## Структура файлов

```
├── config.yaml              # Конфигурация сущностей и пресетов
├── app/
│   ├── llm_engine.py        # GPU-оптимизированный LLM-движок
│   ├── screenplay_parser.py  # Обновленный парсер (использует LLM)
│   └── server.py            # Flask-сервер (обновлен)
└── requirements.txt         # Обновленные зависимости
```

## Использование

### CLI

```bash
# Базовый запуск
python app/screenplay_parser.py -i scenario.docx -o output.xlsx

# С выбором пресета
python app/screenplay_parser.py -i scenario.docx -o output.xlsx --preset basic
python app/screenplay_parser.py -i scenario.docx -o output.xlsx --preset extended
python app/screenplay_parser.py -i scenario.docx -o output.xlsx --preset full

# С кастомным конфигом
python app/screenplay_parser.py -i scenario.docx -o output.xlsx --config custom_config.yaml
```

### Через API (server.py)

Сервер автоматически использует новую LLM-архитектуру. Параметр `template` в форме соответствует пресету:
- `basic` → пресет "basic"
- `extended` → пресет "extended"  
- `full` → пресет "full"

## Конфигурация (config.yaml)

### Пресеты

Пресеты определяют набор извлекаемых сущностей:

- **basic**: Минимальный набор (локация, время, персонажи, синопсис)
- **extended**: Расширенный набор (+ реквизит, транспорт, массовка, VFX)
- **full**: Полный набор всех доступных сущностей

### Типы сущностей

Каждый тип сущности в `config.yaml` содержит:
- `description`: Описание для LLM-промпта
- `type`: Тип данных (string, list, boolean, integer)
- `required`: Обязательность поля

### Настройки LLM

```yaml
llm:
  model_name: "mistralai/Mistral-7B-Instruct-v0.2"  # Модель из HuggingFace
  use_vllm: false  # Использовать vllm (требует CUDA)
  device: "cuda"  # "cuda" или "cpu"
  generation:
    max_new_tokens: 1024
    temperature: 0.1
    top_p: 0.95
  batch:
    scene_detection_batch_size: 10
    entity_extraction_batch_size: 8
```

## Установка зависимостей

```bash
pip install -r requirements.txt
```

Основные зависимости:
- `torch>=2.0.0` - PyTorch для GPU
- `transformers>=4.35.0` - HuggingFace Transformers
- `bitsandbytes>=0.41.0` - Для 4-bit quantization
- `PyYAML>=6.0` - Для чтения конфига
- `vllm>=0.2.0` - Опционально, для максимальной производительности

## Производительность

### Батчевая обработка
- Сцены обрабатываются батчами для эффективного использования GPU
- Размер батча настраивается в `config.yaml`

### Оптимизация памяти
- Автоматическое использование 4-bit quantization при наличии GPU
- Fallback на полную точность при проблемах с quantization

### Поддержка vllm
Для максимальной производительности можно использовать vllm:
1. Установите: `pip install vllm`
2. В `config.yaml` установите `use_vllm: true`

## Кастомизация

### Добавление новой сущности

1. Добавьте описание в `config.yaml`:
```yaml
entity_types:
  my_new_entity:
    description: "Описание новой сущности"
    type: "string"  # или "list", "boolean", "integer"
    required: false
```

2. Добавьте в нужный пресет:
```yaml
presets:
  full:
    entities:
      - my_new_entity
```

3. LLM автоматически начнет извлекать эту сущность!

### Создание кастомного пресета

Добавьте в `config.yaml`:
```yaml
presets:
  my_custom:
    name: "Мой кастомный пресет"
    description: "Описание"
    entities:
      - location
      - characters
      - my_new_entity
```

## Отладка

Включите режим отладки:
```bash
python app/screenplay_parser.py -i scenario.docx -o output.xlsx --debug
```

Логи сохраняются в `screenplay_parser.log`.

## Миграция со старой версии

Старый код с хардкодной эвристикой больше не используется. Все извлечение теперь выполняется через LLM. Если нужно вернуться к старому поведению, используйте старую версию из git истории.

## Требования к модели

Рекомендуемые модели:
- `mistralai/Mistral-7B-Instruct-v0.2` (по умолчанию)
- `meta-llama/Llama-2-7b-chat-hf`
- Любая другая инструктивная модель из HuggingFace

Модель автоматически загружается при первом запуске (требуется интернет).

## Примечания

- Первый запуск может занять время на загрузку модели
- Для работы на GPU требуется CUDA-совместимая видеокарта
- При отсутствии GPU система автоматически переключится на CPU
- Для больших сценариев рекомендуется использовать GPU для приемлемой скорости

