#!/usr/bin/env python3
"""
screenplay_parser.py

–°–µ—Ä–≤–∏—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –≤ excel-—Ç–∞–±–ª–∏—Ü—É.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é LLM —á–µ—Ä–µ–∑ llama-cpp-python –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.

–ê–≤—Ç–æ—Ä: Production Pipeline Parser
–í–µ—Ä—Å–∏—è: 1.0.0
"""

import argparse
import json
import os
import re
import sys
import gc
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict, field
import logging
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
from docx import Document
from tqdm import tqdm
import numpy as np

# –ò–º–ø–æ—Ä—Ç llama-cpp
try:
    from llama_cpp import Llama
except ImportError:
    print("–û—à–∏–±–∫–∞: llama-cpp-python –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install llama-cpp-python")
    # –ù–µ –≤—ã—Ö–æ–¥–∏–º ‚Äî –ø–æ–∑–≤–æ–ª–∏–º —Ä–∞–±–æ—Ç–∞—Ç—å –≤ —Ä–µ–∂–∏–º–µ –±–µ–∑ LLM
    Llama = None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('screenplay_parser.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# -----------------------------
#  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# -----------------------------

class Config:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
    # –ü—É—Ç–∏
    MODEL_PATH = "models/mistral-7b-instruct-v0.2.Q4_K_M.gguf"  # –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –¥–ª—è M3 Pro
    MODEL_PARAMS = {
        'n_ctx': 2048,
        'n_batch': 512,
        'n_threads': 8,
        'n_gpu_layers': 1,
        'use_mmap': True,
        'use_mlock': False,
        'seed': 42,
        'verbose': False
    }
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    GENERATION_PARAMS = {
        'max_tokens': 512,
        'temperature': 0.3,
        'top_p': 0.95,
        'top_k': 40,
        'repeat_penalty': 1.1,
        'stop': ["</s>", "\n\n\n", "---"]
    }
    
    # –ü–∞—Ä—Å–∏–Ω–≥
    MIN_SCENE_LENGTH = 50
    MAX_SCENE_LENGTH = 5000
    BATCH_SIZE = 5  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–æ 5 —Å—Ü–µ–Ω –∑–∞ —Ä–∞–∑


# -----------------------------
#  –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
# -----------------------------

@dataclass
class SceneMetadata:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å—Ü–µ–Ω—ã"""
    scene_number: str = ""
    episode: str = ""
    scene_type: str = ""  # INT/EXT/–ò–ù–¢/–≠–ö–°–¢/–ù–ê–¢
    location: str = ""
    sublocation: str = ""
    time_of_day: str = ""
    synopsis: str = ""
    characters: List[str] = field(default_factory=list)
    extras: str = ""
    extras_count: int = 0
    props: List[str] = field(default_factory=list)
    vehicles: List[str] = field(default_factory=list)
    special_fx: List[str] = field(default_factory=list)
    costumes: List[str] = field(default_factory=list)
    makeup: List[str] = field(default_factory=list)
    stunts: bool = False
    pyrotechnics: bool = False
    special_equipment: List[str] = field(default_factory=list)
    notes: str = ""
    raw_text: str = ""
    confidence_score: float = 0.0


# -----------------------------
#  LLM Manager
# -----------------------------

class LocalLLM:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM"""
    
    def __init__(self, model_path: str = None):
        self.model_path = model_path or Config.MODEL_PATH
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–æ–¥ Mac M3"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {self.model_path}")
            logger.info("–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 1-2 –º–∏–Ω—É—Ç—ã...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏
            if not Path(self.model_path).exists():
                raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.model_path}")
            
            if Llama is None:
                raise RuntimeError("llama-cpp-python –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            self.model = Llama(
                model_path=self.model_path,
                **Config.MODEL_PARAMS
            )
            
            logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            logger.info("–†–∞–±–æ—Ç–∞–µ–º –±–µ–∑ LLM, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –ø—Ä–∞–≤–∏–ª–∞")
            self.model = None
    
    def generate(self, prompt: str, system_prompt: str = None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –ø—Ä–æ–º–ø—Ç"""
        if self.model is None:
            return "{}"
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            if system_prompt:
                full_prompt = f"<s>[INST] {system_prompt}\n\n{prompt} [/INST]"
            else:
                full_prompt = f"<s>[INST] {prompt} [/INST]"
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            response = self.model(
                full_prompt,
                **Config.GENERATION_PARAMS
            )
            
            return response['choices'][0]['text'].strip()
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "{}"
    
    def extract_json(self, text: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏"""
        try:
            # –ò—â–µ–º JSON –≤ —Ç–µ–∫—Å—Ç–µ
            json_match = re.search(r'\{[^}]*\}', text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º common issues
                json_str = json_str.replace("'", '"')
                json_str = re.sub(r',\s*}', '}', json_str)
                json_str = re.sub(r',\s*]', ']', json_str)
                return json.loads(json_str)
        except:
            pass
        return {}
    
    def __del__(self):
        """–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–∞"""
        if self.model:
            try:
                del self.model
            except Exception:
                pass
            gc.collect()


# -----------------------------
#  –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ü–µ–Ω–∞—Ä–∏—è
# -----------------------------

class ScenarioParser:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ü–µ–Ω–∞—Ä–∏—è"""
    
    SCENE_PATTERNS = {
        'heading': re.compile(
            r'^(?P<number>\d+[-.]?\d*\.? )?\s*'
            r'(?P<type>INT\.|EXT\.|–ò–ù–¢\.|–≠–ö–°–¢\.|–ù–ê–¢\.)\s*'
            r'(?P<location>[^.\n]+?)(?:\.\s*(?P<sublocation>[^.\n]+?))?\s*[.\-\s]*\s*'
            r'(?P<time>–î–ï–ù–¨|–ù–û–ß–¨|–£–¢–†–û|–í–ï–ß–ï–†|–†–ê–°–°–í–ï–¢|–ó–ê–ö–ê–¢|–î–µ–Ω—å|–ù–æ—á—å|–£—Ç—Ä–æ|–í–µ—á–µ—Ä)?',
            re.MULTILINE | re.IGNORECASE
        ),
        'character': re.compile(
            r'^([–ê-–Ø–ÅA-Z][–ê-–Ø–ÅA-Z\s\-,]{1,30})(?:\s*\([\w\s,]+\))?$',
            re.MULTILINE
        ),
        'parenthetical': re.compile(
            r'\(([^)]+)\)',
            re.MULTILINE
        )
    }
    
    KEYWORDS = {
        'props': [
            '—Ç–µ–ª–µ—Ñ–æ–Ω', '–Ω–æ—É—Ç–±—É–∫', '–∫–æ–º–ø—å—é—Ç–µ—Ä', '–ø–∏—Å—å–º–æ', '–∫–Ω–∏–≥–∞', '—Å—É–º–∫–∞',
            '–∫–ª—é—á–∏', '–¥–æ–∫—É–º–µ–Ω—Ç—ã', '–æ—Ä—É–∂–∏–µ', '–Ω–æ–∂', '–ø–∏—Å—Ç–æ–ª–µ—Ç', '–¥–µ–Ω—å–≥–∏',
            '—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è', '–∫–∞–º–µ—Ä–∞', '–º–∏–∫—Ä–æ—Ñ–æ–Ω', '–Ω–∞—É—à–Ω–∏–∫–∏', '–æ—á–∫–∏', '—á–∞—Å—ã',
            '–∫–æ–ª—å—Ü–æ', '—Ü–≤–µ—Ç—ã', '–±—É—Ç—ã–ª–∫–∞', '—Å—Ç–∞–∫–∞–Ω', '–µ–¥–∞', '–Ω–∞–ø–∏—Ç–æ–∫'
        ],
        'vehicles': [
            '–º–∞—à–∏–Ω–∞', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', '–∞–≤—Ç–æ–±—É—Å', '—Ç–∞–∫—Å–∏', '–º–æ—Ç–æ—Ü–∏–∫–ª',
            '–≤–µ–ª–æ—Å–∏–ø–µ–¥', '—Å–∞–º–æ–ª–µ—Ç', '–≤–µ—Ä—Ç–æ–ª–µ—Ç', '–ø–æ–µ–∑–¥', '–∫–æ—Ä–∞–±–ª—å', '–ª–æ–¥–∫–∞'
        ],
        'effects': [
            '–≤–∑—Ä—ã–≤', '–≤—ã—Å—Ç—Ä–µ–ª', '–¥—ã–º', '–æ–≥–æ–Ω—å', '–ø–æ–∂–∞—Ä', '–∏—Å–∫—Ä—ã', '–∫—Ä–æ–≤—å',
            '—Å–ª–µ–∑—ã', '–¥–æ–∂–¥—å', '—Å–Ω–µ–≥', '—Ç—É–º–∞–Ω', '–≤–µ—Ç–µ—Ä', '–º–æ–ª–Ω–∏—è', '–≥—Ä–æ–º'
        ],
        'stunts': [
            '–¥—Ä–∞–∫–∞', '—É–¥–∞—Ä', '–ø–∞–¥–µ–Ω–∏–µ', '–ø—Ä—ã–∂–æ–∫', '–ø–æ–≥–æ–Ω—è', '–∞–≤–∞—Ä–∏—è',
            '–±–µ–≥', '–±–æ—Ä—å–±–∞', '—Ç—Ä—é–∫', '–∫–∞—Å–∫–∞–¥–µ—Ä'
        ]
    }
    
    def __init__(self, use_llm: bool = True):
        self.use_llm = use_llm
        self.llm = None
        if use_llm:
            self.llm = LocalLLM()
        self.scenes = []
        
    def parse_screenplay(self, text: str) -> List[SceneMetadata]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ü–µ–Ω–∞—Ä–∏—è"""
        logger.info("–ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ü–µ–Ω–∞—Ä–∏—è...")
        
        scenes_raw = self._split_into_scenes(text)
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(scenes_raw)} —Å—Ü–µ–Ω")
        
        batch_size = Config.BATCH_SIZE
        for i in tqdm(range(0, len(scenes_raw), batch_size), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ü–µ–Ω"):
            batch = scenes_raw[i:i + batch_size]
            
            for j, scene_text in enumerate(batch):
                scene_num = i + j + 1
                metadata = self._extract_scene_metadata(scene_text, scene_num)
                
                if self.use_llm and self.llm and self.llm.model:
                    metadata = self._enhance_with_llm(metadata)
                
                self.scenes.append(metadata)
            
            gc.collect()
        
        return self.scenes
    
    def _split_into_scenes(self, text: str) -> List[str]:
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        
        scenes = []
        scene_headers = list(self.SCENE_PATTERNS['heading'].finditer(text))
        
        if not scene_headers:
            logger.warning("–Ø–≤–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ü–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ")
            parts = re.split(r'\n{2,}', text)
            return [p.strip() for p in parts 
                   if p and Config.MIN_SCENE_LENGTH <= len(p.strip()) <= Config.MAX_SCENE_LENGTH]
        
        for i, match in enumerate(scene_headers):
            start = match.start()
            end = scene_headers[i + 1].start() if i + 1 < len(scene_headers) else len(text)
            scene_text = text[start:end].strip()
            
            if Config.MIN_SCENE_LENGTH <= len(scene_text) <= Config.MAX_SCENE_LENGTH:
                scenes.append(scene_text)
        
        return scenes
    
    def _extract_scene_metadata(self, scene_text: str, scene_num: int) -> SceneMetadata:
        metadata = SceneMetadata(
            scene_number=str(scene_num),
            raw_text=scene_text[:500]
        )
        
        header_match = self.SCENE_PATTERNS['heading'].search(scene_text)
        if header_match:
            groups = header_match.groupdict()
            metadata.scene_number = groups.get('number') or str(scene_num)
            metadata.scene_type = (groups.get('type') or 'INT').strip('.')
            metadata.location = (groups.get('location') or '').strip()
            metadata.sublocation = (groups.get('sublocation') or '').strip()
            metadata.time_of_day = groups.get('time') or '–î–ï–ù–¨'
        
        metadata.characters = self._extract_characters(scene_text)
        metadata.synopsis = self._extract_synopsis(scene_text)
        text_lower = scene_text.lower()
        
        metadata.props = [prop for prop in self.KEYWORDS['props'] 
                         if prop in text_lower][:10]
        
        metadata.vehicles = [v for v in self.KEYWORDS['vehicles'] 
                           if v in text_lower][:5]
        
        metadata.special_fx = [fx for fx in self.KEYWORDS['effects'] 
                              if fx in text_lower]
        
        metadata.stunts = any(stunt in text_lower for stunt in self.KEYWORDS['stunts'])
        
        metadata.pyrotechnics = any(word in text_lower for word in ['–≤–∑—Ä—ã–≤', '–æ–≥–æ–Ω—å', '–ø–æ–∂–∞—Ä', '–≤—ã—Å—Ç—Ä–µ–ª'])
        
        extras_match = re.search(r'(?:–º–∞—Å—Å–æ–≤–∫–∞|—Ç–æ–ª–ø–∞|–∑—Ä–∏—Ç–µ–ª–∏|–ø—Ä–æ—Ö–æ–∂–∏–µ|—Å—Ç—É–¥–µ–Ω—Ç—ã|–≥–æ—Å—Ç–∏)[\s:\-]*(\d+)?', 
                                 text_lower)
        if extras_match:
            metadata.extras = extras_match.group(0)
            if extras_match.group(1):
                try:
                    metadata.extras_count = int(extras_match.group(1))
                except Exception:
                    metadata.extras_count = 0
        
        return metadata
    
    def _extract_characters(self, text: str) -> List[str]:
        characters = set()
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            if self.SCENE_PATTERNS['character'].match(line):
                character = re.sub(r'\([^)]*\)', '', line).strip()
                if (character and 
                    not any(word in character.upper() for word in 
                           ['–ò–ù–¢', '–≠–ö–°–¢', '–ù–ê–¢', '–î–ï–ù–¨', '–ù–û–ß–¨', '–£–¢–†–û', '–í–ï–ß–ï–†']) and
                    len(character) > 2):
                    characters.add(character)
        
        name_contexts = re.findall(
            r'(?:–≥–æ–≤–æ—Ä–∏—Ç|—Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç|–æ—Ç–≤–µ—á–∞–µ—Ç|–∫—Ä–∏—á–∏—Ç|—à–µ–ø—á–µ—Ç|–∑–æ–≤–µ—Ç)\s+([–ê-–Ø–Å][–∞-—è—ë]+)',
            text
        )
        characters.update(name_contexts)
        
        return sorted(list(characters))[:15]
    
    def _extract_synopsis(self, text: str) -> str:
        lines = text.split('\n')
        synopsis_lines = []
        
        start_idx = 0
        for i, line in enumerate(lines):
            if self.SCENE_PATTERNS['heading'].match(line):
                start_idx = i + 1
                break
        
        for line in lines[start_idx:]:
            line = line.strip()
            if self.SCENE_PATTERNS['character'].match(line):
                break
            if line and not line.isupper():
                synopsis_lines.append(line)
                if len(' '.join(synopsis_lines)) > 300:
                    break
        
        synopsis = ' '.join(synopsis_lines)
        synopsis = ' '.join(synopsis.split())
        
        return synopsis[:400]
    
    def _enhance_with_llm(self, metadata: SceneMetadata) -> SceneMetadata:
        if not self.llm or not self.llm.model:
            return metadata
        
        try:
            system_prompt = """–¢—ã - –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–µ–∂–∏—Å—Å–µ—Ä–∞, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π —Å—Ü–µ–Ω–∞—Ä–∏–∏ –¥–ª—è –∫–∏–Ω–æ–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∏–∑–≤–ª–µ—á—å —Ç–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö —Å—Ü–µ–Ω—ã.
–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""

            prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—Ü–µ–Ω—É –∏ –∏–∑–≤–ª–µ–∫–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:

–°–¶–ï–ù–ê: {metadata.location} - {metadata.time_of_day}
–¢–ï–ö–°–¢: {metadata.raw_text}

–£–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω–æ:
- –ü–µ—Ä—Å–æ–Ω–∞–∂–∏: {', '.join(metadata.characters[:5]) if metadata.characters else '–Ω–µ –Ω–∞–π–¥–µ–Ω—ã'}
- –†–µ–∫–≤–∏–∑–∏—Ç: {', '.join(metadata.props[:5]) if metadata.props else '–Ω–µ –Ω–∞–π–¥–µ–Ω'}

–î–æ–ø–æ–ª–Ω–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
  "extras_description": "–æ–ø–∏—Å–∞–Ω–∏–µ –º–∞—Å—Å–æ–≤–∫–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ",
  "additional_props": ["–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π", "—Ä–µ–∫–≤–∏–∑–∏—Ç"],
  "costume_notes": ["–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏", "–∫–æ—Å—Ç—é–º–æ–≤"],
  "makeup_notes": ["—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "–∫ –≥—Ä–∏–º—É"],
  "special_requirements": "–æ—Å–æ–±—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å—ä–µ–º–∫–µ"
}}"""

            response = self.llm.generate(prompt, system_prompt)
            data = self.llm.extract_json(response)
            
            if data:
                if 'extras_description' in data:
                    metadata.extras = str(data['extras_description'])
                
                if 'additional_props' in data and isinstance(data['additional_props'], list):
                    metadata.props.extend(data['additional_props'])
                    metadata.props = list(set(metadata.props))[:15]
                    
                if 'costume_notes' in data and isinstance(data['costume_notes'], list):
                    metadata.costumes = data['costume_notes'][:5]
                    
                if 'makeup_notes' in data and isinstance(data['makeup_notes'], list):
                    metadata.makeup = data['makeup_notes'][:5]
                    
                if 'special_requirements' in data:
                    metadata.notes = str(data['special_requirements'])
                
                metadata.confidence_score = 0.8
            else:
                metadata.confidence_score = 0.5
                
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ —Å LLM: {e}")
            metadata.confidence_score = 0.5
        
        return metadata


# -----------------------------
#  –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
# -----------------------------

def read_docx(path: str) -> str:
    """–ß–∏—Ç–∞–µ—Ç .docx —Ñ–∞–π–ª"""
    try:
        doc = Document(path)
        paragraphs = []
        
        for para in doc.paragraphs:
            text = para.text.strip()
            if text:
                if getattr(para.style, 'name', '').startswith('Heading'):
                    text = f"\n\n{text}\n"
                paragraphs.append(text)
        
        return "\n".join(paragraphs)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {path}: {e}")
        raise


# -----------------------------
#  –≠–∫—Å–ø–æ—Ä—Ç –≤ Excel
# -----------------------------

def create_production_table(scenes: List[SceneMetadata]) -> pd.DataFrame:
    rows = []
    
    for scene in scenes:
        row = {
            "–°–µ—Ä–∏—è": scene.episode or "01",
            "–°—Ü–µ–Ω–∞": scene.scene_number,
            "–†–µ–∂–∏–º": scene.time_of_day,
            "–ò–Ω—Ç/–ù–∞—Ç": scene.scene_type,
            "–û–±—ä–µ–∫—Ç": scene.location,
            "–ü–æ–¥–æ–±—ä–µ–∫—Ç": scene.sublocation,
            "–°–∏–Ω–æ–ø—Å–∏—Å": scene.synopsis[:200] if scene.synopsis else "",
            "–ü–µ—Ä—Å–æ–Ω–∞–∂–∏": ", ".join(scene.characters[:8]) if scene.characters else "",
            "–ú–∞—Å—Å–æ–≤–∫–∞": scene.extras,
            "–ö–æ–ª-–≤–æ –º–∞—Å—Å–æ–≤–∫–∏": scene.extras_count if scene.extras_count else "",
            "–†–µ–∫–≤–∏–∑–∏—Ç": ", ".join(scene.props[:8]) if scene.props else "",
            "–ò–≥—Ä–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç": ", ".join(scene.vehicles) if scene.vehicles else "",
            "–•—É–¥–æ–∂–Ω–∏–∫–∏": "",
            "–ì—Ä–∏–º": ", ".join(scene.makeup) if scene.makeup else "",
            "–ö–æ—Å—Ç—é–º": ", ".join(scene.costumes) if scene.costumes else "",
            "–ö–∞—Å–∫–∞–¥–µ—Ä—ã": "–î–∞" if scene.stunts else "",
            "–ü–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞": "–î–∞" if scene.pyrotechnics else "",
            "–°–ø–µ—Ü. –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ": ", ".join(scene.special_equipment) if scene.special_equipment else "",
            "–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ": scene.notes,
            "–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å": f"{scene.confidence_score:.0%}" if scene.confidence_score > 0 else ""
        }
        rows.append(row)
    
    df = pd.DataFrame(rows)
    
    try:
        df['scene_num'] = df['–°—Ü–µ–Ω–∞'].astype(str).str.extract(r'(\d+)').astype(float)
        df = df.sort_values('scene_num').drop('scene_num', axis=1)
    except Exception:
        pass
    
    return df


def export_to_excel(df: pd.DataFrame, output_path: str):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç DataFrame –≤ Excel —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    try:
        from openpyxl import Workbook
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        from openpyxl.utils.dataframe import dataframe_to_rows
        
        wb = Workbook()
        ws = wb.active
        ws.title = "–ö–ü–ü"
        
        ws.append([f"–ö–ü–ü - –ö–∞–ª–µ–Ω–¥–∞—Ä–Ω–æ-–ø–æ—Å—Ç–∞–Ω–æ–≤–æ—á–Ω—ã–π –ø–ª–∞–Ω"])
        ws.merge_cells('A1:T1')
        
        header_font = Font(bold=True, size=14)
        ws['A1'].font = header_font
        ws['A1'].alignment = Alignment(horizontal='center')
        
        ws.append([])
        
        for r in dataframe_to_rows(df, index=False, header=True):
            ws.append(r)
        
        header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF")
        
        for cell in ws[3]:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)
        
        column_widths = {
            'A': 8, 'B': 10, 'C': 10, 'D': 12, 'E': 25, 'F': 25, 'G': 40,
            'H': 30, 'I': 20, 'J': 10, 'K': 30, 'L': 20, 'M': 20, 'N': 20,
            'O': 20, 'P': 12, 'Q': 12, 'R': 25, 'S': 30, 'T': 12
        }
        
        for col, width in column_widths.items():
            ws.column_dimensions[col].width = width
        
        thin_border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )
        
        for row in ws.iter_rows(min_row=3, max_row=ws.max_row):
            for cell in row:
                cell.border = thin_border
                cell.alignment = Alignment(vertical='top', wrap_text=True)
        
        for row in ws.iter_rows(min_row=4, max_row=ws.max_row, min_col=20, max_col=20):
            for cell in row:
                if cell.value:
                    try:
                        confidence = float(str(cell.value).strip('%')) / 100
                        if confidence >= 0.8:
                            cell.fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
                        elif confidence >= 0.6:
                            cell.fill = PatternFill(start_color="FFEB9C", end_color="FFEB9C", fill_type="solid")
                        else:
                            cell.fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
                    except:
                        pass
        
        wb.save(output_path)
        logger.info(f"–¢–∞–±–ª–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ {output_path}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –≤ Excel: {e}")
        df.to_excel(output_path, index=False, sheet_name='–ö–ü–ü')
        logger.info(f"–¢–∞–±–ª–∏—Ü–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ")


# -----------------------------
#  –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –æ—Ç—á–µ—Ç—ã
# -----------------------------

def print_statistics(scenes: List[SceneMetadata], df: pd.DataFrame):
    print("\n" + "="*70)
    print(" " * 20 + "–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ò–ù–ì–ê")
    print("="*70)
    
    print(f"\nüìä –û–°–ù–û–í–ù–´–ï –ü–û–ö–ê–ó–ê–¢–ï–õ–ò:")
    print(f"  ‚Ä¢ –í—Å–µ–≥–æ —Å—Ü–µ–Ω: {len(scenes)}")
    print(f"  ‚Ä¢ –ò–Ω—Ç–µ—Ä—å–µ—Ä—ã: {sum(1 for s in scenes if s.scene_type in ['INT', '–ò–ù–¢'])}")
    print(f"  ‚Ä¢ –ù–∞—Ç—É—Ä–∞: {sum(1 for s in scenes if s.scene_type in ['EXT', '–≠–ö–°–¢', '–ù–ê–¢'])}")
    print(f"  ‚Ä¢ –î–Ω–µ–≤–Ω—ã–µ —Å—Ü–µ–Ω—ã: {sum(1 for s in scenes if '–î–ï–ù–¨' in s.time_of_day.upper())}")
    print(f"  ‚Ä¢ –ù–æ—á–Ω—ã–µ —Å—Ü–µ–Ω—ã: {sum(1 for s in scenes if '–ù–û–ß–¨' in s.time_of_day.upper())}")
    
    locations = df['–û–±—ä–µ–∫—Ç'].value_counts() if '–û–±—ä–µ–∫—Ç' in df.columns else pd.Series()
    print(f"\nüìç –õ–û–ö–ê–¶–ò–ò:")
    print(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ–∫–∞—Ü–∏–π: {len(locations)}")
    print(f"  ‚Ä¢ –¢–æ–ø-5 –ª–æ–∫–∞—Ü–∏–π:")
    for loc, count in locations.head(5).items():
        print(f"    - {loc}: {count} —Å—Ü–µ–Ω")
    
    all_characters = []
    for scene in scenes:
        all_characters.extend(scene.characters)
    unique_chars = list(set(all_characters))
    
    print(f"\nüë• –ü–ï–†–°–û–ù–ê–ñ–ò:")
    print(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π: {len(unique_chars)}")
    if unique_chars:
        from collections import Counter
        char_counts = Counter(all_characters)
        print(f"  ‚Ä¢ –¢–æ–ø-5 –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π:")
        for char, count in char_counts.most_common(5):
            print(f"    - {char}: {count} —Å—Ü–µ–Ω")
    
    print(f"\nüé¨ –ü–†–û–ò–ó–í–û–î–°–¢–í–ï–ù–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:")
    print(f"  ‚Ä¢ –°—Ü–µ–Ω—ã —Å –º–∞—Å—Å–æ–≤–∫–æ–π: {sum(1 for s in scenes if s.extras)}")
    print(f"  ‚Ä¢ –°—Ü–µ–Ω—ã —Å —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º: {sum(1 for s in scenes if s.vehicles)}")
    print(f"  ‚Ä¢ –°—Ü–µ–Ω—ã —Å–æ —Å–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏: {sum(1 for s in scenes if s.special_fx)}")
    print(f"  ‚Ä¢ –°—Ü–µ–Ω—ã —Å —Ç—Ä—é–∫–∞–º–∏: {sum(1 for s in scenes if s.stunts)}")
    print(f"  ‚Ä¢ –°—Ü–µ–Ω—ã —Å –ø–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–æ–π: {sum(1 for s in scenes if s.pyrotechnics)}")
    
    if any(s.confidence_score > 0 for s in scenes):
        avg_confidence = np.mean([s.confidence_score for s in scenes if s.confidence_score > 0])
        print(f"\nüìà –ö–ê–ß–ï–°–¢–í–û –ü–ê–†–°–ò–ù–ì–ê:")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.0%}")
        print(f"  ‚Ä¢ –°—Ü–µ–Ω—ã —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (>80%): {sum(1 for s in scenes if s.confidence_score > 0.8)}")
        print(f"  ‚Ä¢ –°—Ü–µ–Ω—ã —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (<60%): {sum(1 for s in scenes if 0 < s.confidence_score < 0.6)}")
    
    print("\n" + "="*70)


def main():
    parser = argparse.ArgumentParser(
        description="üé¨ –ü–∞—Ä—Å–µ—Ä —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ö–ü–ü (–∫–∞–ª–µ–Ω–¥–∞—Ä–Ω–æ-–ø–æ—Å—Ç–∞–Ω–æ–≤–æ—á–Ω–æ–≥–æ –ø–ª–∞–Ω–∞)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python screenplay_parser.py -i scenario.docx -o production.xlsx
  python screenplay_parser.py -i scenario.docx --no-llm  # –±–µ–∑ LLM
  python screenplay_parser.py -i scenario.docx --model models/my_model.gguf
        """
    )
    
    parser.add_argument(
        "--input", "-i",
        required=True,
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å—Ü–µ–Ω–∞—Ä–∏—è (.docx)"
    )
    parser.add_argument(
        "--output", "-o",
        default="production_table.xlsx",
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è Excel —Ç–∞–±–ª–∏—Ü—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: production_table.xlsx)"
    )
    parser.add_argument(
        "--model",
        default=Config.MODEL_PATH,
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏ GGUF"
    )
    parser.add_argument(
        "--no-llm",
        action="store_true",
        help="–ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM (—Ç–æ–ª—å–∫–æ –ø—Ä–∞–≤–∏–ª–∞)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=Config.BATCH_SIZE,
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ü–µ–Ω"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="–†–µ–∂–∏–º –æ—Ç–ª–∞–¥–∫–∏ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –≤—ã–≤–æ–¥–æ–º"
    )
    
    args = parser.parse_args()
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    if not os.path.exists(args.input):
        logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.input}")
        sys.exit(1)
    
    if not args.input.endswith('.docx'):
        logger.error("‚ùå –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ñ–æ—Ä–º–∞—Ç .docx")
        sys.exit(1)
    
    Config.MODEL_PATH = args.model
    Config.BATCH_SIZE = args.batch_size
    
    try:
        logger.info(f"üìñ –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ {args.input}...")
        text = read_docx(args.input)
        logger.info(f"‚úì –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        use_llm = not args.no_llm
        if use_llm:
            logger.info("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM...")
        else:
            logger.info("üìù –†–∞–±–æ—Ç–∞ –≤ —Ä–µ–∂–∏–º–µ –±–µ–∑ LLM (—Ç–æ–ª—å–∫–æ –ø—Ä–∞–≤–∏–ª–∞)")
        
        parser_obj = ScenarioParser(use_llm=use_llm)
        
        scenes = parser_obj.parse_screenplay(text)
        logger.info(f"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ü–µ–Ω: {len(scenes)}")
        
        logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã production...")
        df = create_production_table(scenes)
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {args.output}...")
        export_to_excel(df, args.output)
        
        print_statistics(scenes, df)
        
        print(f"\n‚úÖ –ì–û–¢–û–í–û! –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {args.output}")
        print(f"üìÇ –û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª –≤ Excel –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(0)
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        sys.exit(1)
    finally:
        gc.collect()


if __name__ == "__main__":
    main()
